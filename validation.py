"""
Ontology validation module for CyberRAG project.
This module provides functions to validate answers against an ontology using LLM.
"""

import os
import torch
from torch.utils.data import DataLoader
from utils import QADataset, remove_prompt


def answer_check(answer):
    """
    Check if an answer is a rejection/negative response.
    
    Args:
        answer: Answer string to check
    
    Returns:
        False if answer is a rejection word, True otherwise
    """
    answer = answer.lower().strip()
    reject_words = [
        'no', 'not', 'none', 'nothing', 'nope', 'nada', 'nah', 'never', 
        'negative', 'n', 'nil', 'nix', 'nought', 'zero', 'zilch', 'zip', 
        'zippo', 'zot', 'zotz'
    ]
    if answer in reject_words:
        return False
    else:
        return True


def answer_filter(answer, response_content):
    """
    Filter answer based on response content validation.
    
    Args:
        answer: Original answer
        response_content: Response content from validation
    
    Returns:
        Filtered answer (original answer if response is positive, response_content otherwise)
    """
    if answer_check(response_content):
        return answer
    else:
        return response_content


def build_validation_prompt(question, answer, ontology_text):
    """
    Build prompt for ontology validation.
    
    Args:
        question: Original question
        answer: Generated answer that needs to be evaluated
        ontology_text: Ontology knowledge base
    
    Returns:
        input_text: Formatted prompt for the model
    """
    # Prepare instruction
    instruction = (
        'Please judge if the QUESTION and ANSWER align well with the ONTOLOGY.\n'
        'The QUESTION and ANSWER align well with the ONTOLOGY if the QUESTION and ANSWER '
        'are in the same knowledge domain as the ONTOLOGY.\n'
        'The output format is a tuple: (your judgment: Pass/Not Pass, confidence score)'
    )
    
    # Format input text
    input_text = (
        f'QUESTION:\n{question}\n\n'
        f'ANSWER:\n{answer}\n\n'
        f'ONTOLOGY:\n{ontology_text}\n\n'
        f'INSTRUCTIONS:\n{instruction}'
    )
    return input_text


def ontology_validation(question, answer, model, tokenizer, ontology_path='dataset/ontology/', 
                        device=None, batch_size=1):
    """
    Validate if a question and answer align well with the ontology using local model.
    
    Args:
        question: The question that was queried
        answer: The answer generated by LLM
        model: Loaded language model
        tokenizer: Model tokenizer
        ontology_path: Path to the ontology directory (default: 'dataset/ontology/')
        device: Computing device (cuda/cpu), if None will auto-detect
        batch_size: Batch size for inference (default: 1)
    
    Returns:
        Validation result (raw prediction text from model)
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Read ontology file
    ontology_file = os.path.join(ontology_path, 'ontology.txt')
    with open(ontology_file, 'r') as f:
        ontology_text = f.read()
    
    # Build prompt
    input_text = build_validation_prompt(question, answer, ontology_text)
    
    # Create dataset and dataloader
    dataset = QADataset([input_text], [0], tokenizer, max_len=1024)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    # Run inference
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            outputs = model.generate(
                input_ids=input_ids, 
                attention_mask=attention_mask, 
                max_new_tokens=256, 
                num_beams=4
            )
            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            # Remove prompt from prediction
            validation_result = remove_prompt([input_text], decoded_preds)[0]
            return validation_result


def validate_batch(questions, answers, model, tokenizer, ontology_path='dataset/ontology/', 
                   device=None, batch_size=1):
    """
    Validate a batch of question-answer pairs against the ontology using local model.
    
    Args:
        questions: List of questions
        answers: List of answers (should match length of questions)
        model: Loaded language model
        tokenizer: Model tokenizer
        ontology_path: Path to the ontology directory
        device: Computing device (cuda/cpu), if None will auto-detect
        batch_size: Batch size for inference (default: 1, recommended for validation)
    
    Returns:
        List of validation results (raw prediction texts)
    """
    if len(questions) != len(answers):
        raise ValueError("Questions and answers must have the same length")
    
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Read ontology file once
    ontology_file = os.path.join(ontology_path, 'ontology.txt')
    with open(ontology_file, 'r') as f:
        ontology_text = f.read()
    
    # Build prompts for all samples
    prompts = []
    for question, answer in zip(questions, answers):
        prompt = build_validation_prompt(question, answer, ontology_text)
        prompts.append(prompt)
    
    # Create dataset and dataloader
    dummy_answers = list(range(len(prompts)))
    dataset = QADataset(prompts, dummy_answers, tokenizer, max_len=1024)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    # Run batch inference
    validation_results = []
    with torch.no_grad():
        for batch_idx, batch in enumerate(data_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            outputs = model.generate(
                input_ids=input_ids, 
                attention_mask=attention_mask, 
                max_new_tokens=256, 
                num_beams=4
            )
            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            
            # Get prompts for this batch
            batch_prompts = prompts[batch_idx * batch_size:(batch_idx + 1) * batch_size]
            
            # Remove prompt from each prediction
            batch_results = remove_prompt(batch_prompts, decoded_preds)
            validation_results.extend(batch_results)
    
    return validation_results
